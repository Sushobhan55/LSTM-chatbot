{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "31196d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6c5c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/movie_lines.txt',  encoding='utf-8', errors ='ignore').read().split('\\n')\n",
    "conversations_lines = open('data/movie_conversations.txt',  encoding='utf-8', errors ='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a73eb1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33804dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "621473e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with key = id and value = text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    id2line[_line[0]] = _line[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd6fbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all conversations' lines\n",
    "# lsit of lists\n",
    "conversations = [ ]\n",
    "for line in conversations_lines:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conversations.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "863affa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f194d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the lines into inupt texts and output texts\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation)-1):\n",
    "        inputs.append(id2line[conversation[i]])\n",
    "        outputs.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "502bd36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load a couple of sorted input-output texts\n",
    "for i in range(5):\n",
    "    print(inputs[i])\n",
    "    print(outputs[i]+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "387419a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e0065099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change word format and remove unnecessary characters\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5cc0d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "clean_inputs = []\n",
    "for text in inputs:\n",
    "    clean_inputs.append(clean_text(text))\n",
    "    \n",
    "clean_outputs = []    \n",
    "for text in outputs:\n",
    "    clean_outputs.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6821c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "not the hacking and gagging and spitting part  please\n",
      "\n",
      "not the hacking and gagging and spitting part  please\n",
      "okay then how about we try out some french cuisine  saturday  night\n",
      "\n",
      "you are asking me out  that is so cute that is your name again\n",
      "forget it\n",
      "\n",
      "no no it is my fault  we did not have a proper introduction \n",
      "cameron\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load a couple of sorted input-output texts\n",
    "for i in range(5):\n",
    "    print(clean_inputs[i])\n",
    "    print(clean_outputs[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b544a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for text in clean_inputs:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for text in clean_outputs:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "466cb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rare words from the vocabulary.\n",
    "# aim to replace fewer than 5% of words with <UNK>\n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c205aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 68433\n",
      "Size of vocab we will use: 13978\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "13405859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to use a different vocabulary sizes for the source and target text, \n",
    "# set different threshold values.\n",
    "# create dictionaries to provide a unique integer for each word.\n",
    "inputs_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        inputs_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "outputs_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        outputs_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb744562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the unique codes to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    inputs_vocab_to_int[code] = len(inputs_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    outputs_vocab_to_int[code] = len(outputs_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "47474bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to map the unique integers to their respective words.\n",
    "inputs_int_to_vocab = {i: v for v, i in inputs_vocab_to_int.items()}\n",
    "outputs_int_to_vocab = {o: v for v, o in outputs_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2b10b915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13982\n",
      "13979\n",
      "13982\n",
      "13982\n"
     ]
    }
   ],
   "source": [
    "# the length of the dictionaries.\n",
    "print(len(inputs_vocab_to_int))\n",
    "print(len(inputs_int_to_vocab))\n",
    "print(len(outputs_vocab_to_int))\n",
    "print(len(outputs_int_to_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4ff41f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the end of sentence token to the end of every answer.\n",
    "for i in range(len(clean_outputs)):\n",
    "    clean_outputs[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b148f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to integers. \n",
    "# replace any words that are not in the vocabulary with <UNK> \n",
    "inputs_int = []\n",
    "for text in clean_inputs:\n",
    "    ints = []\n",
    "    for word in text.split():\n",
    "        if word not in inputs_vocab_to_int:\n",
    "            ints.append(inputs_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(inputs_vocab_to_int[word])\n",
    "    inputs_int.append(ints)\n",
    "    \n",
    "outputs_int = []\n",
    "for text in clean_outputs:\n",
    "    ints = []\n",
    "    for word in text.split():\n",
    "        if word not in outputs_vocab_to_int:\n",
    "            ints.append(outputs_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(outputs_vocab_to_int[word])\n",
    "    outputs_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "82b93c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs_int))\n",
    "print(len(outputs_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a9ba703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 5040476\n",
      "Number of times <UNK> is used: 140212\n",
      "Percent of words that are <UNK>: 2.78%\n"
     ]
    }
   ],
   "source": [
    "# calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for text in inputs_int:\n",
    "    for word in text:\n",
    "        if word == inputs_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for text in outputs_int:\n",
    "    for word in text:\n",
    "        if word == outputs_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0c5a2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e6552f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5da32b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    '''Create the encoding layer'''\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n",
    "                                                   cell_bw = enc_cell,\n",
    "                                                   sequence_length = sequence_length,\n",
    "                                                   inputs = rnn_inputs, \n",
    "                                                   dtype=tf.float32)\n",
    "    return enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "98c83591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    '''Decode the training data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                     att_keys,\n",
    "                                                                     att_vals,\n",
    "                                                                     att_score_fn,\n",
    "                                                                     att_construct_fn,\n",
    "                                                                     name = \"attn_dec_train\")\n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                              train_decoder_fn, \n",
    "                                                              dec_embed_input, \n",
    "                                                              sequence_length, \n",
    "                                                              scope=decoding_scope)\n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9426680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob, batch_size):\n",
    "    '''Decode the prediction data'''\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "            tf.contrib.seq2seq.prepare_attention(attention_states,\n",
    "                                                 attention_option=\"bahdanau\",\n",
    "                                                 num_units=dec_cell.output_size)\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, \n",
    "                                                                         encoder_state[0], \n",
    "                                                                         att_keys, \n",
    "                                                                         att_vals, \n",
    "                                                                         att_score_fn, \n",
    "                                                                         att_construct_fn, \n",
    "                                                                         dec_embeddings,\n",
    "                                                                         start_of_sequence_id, \n",
    "                                                                         end_of_sequence_id, \n",
    "                                                                         maximum_length, \n",
    "                                                                         vocab_size, \n",
    "                                                                         name = \"attn_dec_inf\")\n",
    "    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, \n",
    "                                                                infer_decoder_fn, \n",
    "                                                                scope=decoding_scope)\n",
    "    \n",
    "    return infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a02f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, vocab_to_int, keep_prob, batch_size):\n",
    "    '''Create the decoding cell and input the parameters for the training and inference decoding layers'''\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "        \n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope,\n",
    "                                                                weights_initializer = weights,\n",
    "                                                                biases_initializer = biases)\n",
    "\n",
    "        train_logits = decoding_layer_train(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embed_input, \n",
    "                                            sequence_length, \n",
    "                                            decoding_scope, \n",
    "                                            output_fn, \n",
    "                                            keep_prob, \n",
    "                                            batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        infer_logits = decoding_layer_infer(encoder_state, \n",
    "                                            dec_cell, \n",
    "                                            dec_embeddings, \n",
    "                                            vocab_to_int['<GO>'],\n",
    "                                            vocab_to_int['<EOS>'], \n",
    "                                            sequence_length - 1, \n",
    "                                            vocab_size,\n",
    "                                            decoding_scope, \n",
    "                                            output_fn, keep_prob, \n",
    "                                            batch_size)\n",
    "\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b998fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, \n",
    "                  questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  questions_vocab_to_int):\n",
    "    \n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n",
    "                                                       answers_vocab_size+1, \n",
    "                                                       enc_embedding_size,\n",
    "                                                       initializer = tf.random_uniform_initializer(0,1))\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "\n",
    "    dec_input = process_encoding_input(target_data, questions_vocab_to_int, batch_size)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([questions_vocab_size+1, dec_embedding_size], 0, 1))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    train_logits, infer_logits = decoding_layer(dec_embed_input, \n",
    "                                                dec_embeddings, \n",
    "                                                enc_state, \n",
    "                                                questions_vocab_size, \n",
    "                                                sequence_length, \n",
    "                                                rnn_size, \n",
    "                                                num_layers, \n",
    "                                                questions_vocab_to_int, \n",
    "                                                keep_prob, \n",
    "                                                batch_size)\n",
    "    return train_logits, infer_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "437e5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the Hyperparameters\n",
    "max_line_length = 20\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "rnn_size = 512\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e92e4a24",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-783c15367f3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Create the training and inference logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m train_logits, inference_logits = seq2seq_model(\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_vocab_to_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_vocab_to_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding_embedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoding_embedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-138-1f9d65be511a>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[1;34m(input_data, target_data, keep_prob, batch_size, sequence_length, answers_vocab_size, questions_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, questions_vocab_to_int)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'''Use the previous functions to create the training and inference logits'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     enc_embed_input = tf.contrib.layers.embed_sequence(input_data, \n\u001b[0m\u001b[0;32m      8\u001b[0m                                                        \u001b[0manswers_vocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                                        \u001b[0menc_embedding_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "# Reset the graph to ensure that it is ready for training\n",
    "tf.reset_default_graph()\n",
    "# Start the session\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "# Load the model inputs    \n",
    "input_data, targets, lr, keep_prob = model_inputs()\n",
    "# Sequence length will be the max line length for each batch\n",
    "sequence_length = tf.placeholder_with_default(max_line_length, None, name='sequence_length')\n",
    "# Find the shape of the input data for sequence_loss\n",
    "input_shape = tf.shape(input_data)\n",
    "\n",
    "# Create the training and inference logits\n",
    "train_logits, inference_logits = seq2seq_model(\n",
    "    tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(inputs_vocab_to_int), \n",
    "    len(inputs_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n",
    "    outputs_vocab_to_int)\n",
    "\n",
    "# Create a tensor for the inference logits, needed if loading a checkpoint version of the model\n",
    "tf.identity(inference_logits, 'logits')\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        train_logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beee1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d6c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b012c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf019d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
