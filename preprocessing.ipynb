{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31196d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c5c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/movie_lines.txt',  encoding='utf-8', errors ='ignore').read().split('\\n')\n",
    "conversations_lines = open('data/movie_conversations.txt',  encoding='utf-8', errors ='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73eb1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33804dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621473e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with key = id and value = text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6fbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all conversations' lines\n",
    "# lsit of lists\n",
    "conversations = [ ]\n",
    "for line in conversations_lines:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conversations.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863affa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f194d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the lines into inupt texts and output texts\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation)-1):\n",
    "        inputs.append(id2line[conversation[i]])\n",
    "        outputs.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502bd36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load a couple of sorted input-output texts\n",
    "for i in range(5):\n",
    "    print(inputs[i])\n",
    "    print(outputs[i]+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "387419a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0065099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change word format and remove unnecessary characters\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc0d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "clean_inputs = []\n",
    "for text in inputs:\n",
    "    clean_inputs.append(clean_text(text))\n",
    "    \n",
    "clean_outputs = []    \n",
    "for text in outputs:\n",
    "    clean_outputs.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6821c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "not the hacking and gagging and spitting part  please\n",
      "\n",
      "not the hacking and gagging and spitting part  please\n",
      "okay then how about we try out some french cuisine  saturday  night\n",
      "\n",
      "you are asking me out  that is so cute that is your name again\n",
      "forget it\n",
      "\n",
      "no no it is my fault  we did not have a proper introduction \n",
      "cameron\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load a couple of sorted input-output texts\n",
    "for i in range(5):\n",
    "    print(clean_inputs[i])\n",
    "    print(clean_outputs[i]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac387c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the length of sentences\n",
    "lengths = []\n",
    "for text in clean_inputs:\n",
    "    lengths.append(len(text.split()))\n",
    "for text in clean_outputs:\n",
    "    lengths.append(len(text.split()))\n",
    "\n",
    "# create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a64737b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>443232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.872094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.215895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>555.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              counts\n",
       "count  443232.000000\n",
       "mean       10.872094\n",
       "std        12.215895\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         7.000000\n",
       "75%        14.000000\n",
       "max       555.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf79f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n",
      "58.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))\n",
    "print(np.percentile(lengths, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d3b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove input and output texts that are shorter than 2 words and longer than 10 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 10\n",
    "\n",
    "# filter out the input texts that are too short/long\n",
    "short_inputs_temp = []\n",
    "short_outputs_temp = []\n",
    "\n",
    "i = 0\n",
    "for text in clean_inputs:\n",
    "    if len(text.split()) >= min_line_length and len(text.split()) <= max_line_length:\n",
    "        short_inputs_temp.append(text)\n",
    "        short_outputs_temp.append(clean_outputs[i])\n",
    "    i += 1\n",
    "\n",
    "# filter out the output texts that are too short/long\n",
    "short_inputs = []\n",
    "short_outputs = []\n",
    "\n",
    "i = 0\n",
    "for text in short_outputs_temp:\n",
    "    if len(text.split()) >= min_line_length and len(text.split()) <= max_line_length:\n",
    "        short_outputs.append(text)\n",
    "        short_inputs.append(short_inputs_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fefcdd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of questions: 74473\n",
      "# of answers: 74473\n",
      "% of data used: 34.0%\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of lines we will use with the total number of lines.\n",
    "print('# of questions:', len(short_inputs))\n",
    "print('# of answers:', len(short_outputs))\n",
    "print('% of data used: {}%'.format(round(len(short_inputs)/len(inputs),2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b544a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for text in short_inputs:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for text in short_outputs:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "466cb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rare words from the vocabulary.\n",
    "# aim to replace fewer than 5% of words with <UNK>\n",
    "threshold = 10\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c205aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 27254\n",
      "Size of vocab we will use: 3875\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13405859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we want to use a different vocabulary sizes for the source and target text, \n",
    "# set different threshold values.\n",
    "# create dictionaries to provide a unique integer for each word.\n",
    "inputs_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        inputs_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "outputs_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        outputs_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb744562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the unique codes to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    inputs_vocab_to_int[code] = len(inputs_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    outputs_vocab_to_int[code] = len(outputs_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4f85011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3879\n",
      "3878\n",
      "3879\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(outputs_vocab_to_int['<GO>'])\n",
    "print(outputs_vocab_to_int['<UNK>'])\n",
    "print(inputs_vocab_to_int['<GO>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47474bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries to map the unique integers to their respective words.\n",
    "# reverse of vocab_to_int\n",
    "inputs_int_to_vocab = {i: v for v, i in inputs_vocab_to_int.items()}\n",
    "outputs_int_to_vocab = {o: v for v, o in outputs_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b10b915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3879\n",
      "3879\n",
      "3879\n",
      "3879\n"
     ]
    }
   ],
   "source": [
    "# the length of the dictionaries.\n",
    "print(len(inputs_vocab_to_int))\n",
    "print(len(inputs_int_to_vocab))\n",
    "print(len(outputs_vocab_to_int))\n",
    "print(len(outputs_int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ff41f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_outputs)):\n",
    "    short_outputs[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b148f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to integers. \n",
    "# replace any words that are not in the vocabulary with <UNK> \n",
    "inputs_int = []\n",
    "for text in short_inputs:\n",
    "    ints = []\n",
    "    for word in text.split():\n",
    "        if word not in inputs_vocab_to_int:\n",
    "            ints.append(inputs_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(inputs_vocab_to_int[word])\n",
    "    inputs_int.append(ints)\n",
    "    \n",
    "outputs_int = []\n",
    "for text in short_outputs:\n",
    "    ints = []\n",
    "    for word in text.split():\n",
    "        if word not in outputs_vocab_to_int:\n",
    "            ints.append(outputs_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(outputs_vocab_to_int[word])\n",
    "    outputs_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82b93c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74473\n",
      "74473\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs_int))\n",
    "print(len(outputs_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a9ba703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 887697\n",
      "Number of times <UNK> is used: 54389\n",
      "Percent of words that are <UNK>: 6.13%\n"
     ]
    }
   ],
   "source": [
    "# calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for text in inputs_int:\n",
    "    for word in text:\n",
    "        if word == inputs_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for text in outputs_int:\n",
    "    for word in text:\n",
    "        if word == outputs_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "unk_ratio = round(unk_count/word_count,4)*100\n",
    "    \n",
    "print(\"Total number of words:\", word_count)\n",
    "print(\"Number of times <UNK> is used:\", unk_count)\n",
    "print(\"Percent of words that are <UNK>: {}%\".format(round(unk_ratio,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e548bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74473, 10) 10\n"
     ]
    }
   ],
   "source": [
    "# padding encoder_input_data\n",
    "padded_inputs = preprocessing.sequence.pad_sequences(inputs_int , maxlen=max_line_length , padding='post')\n",
    "encoder_input_data = np.array(padded_inputs)\n",
    "print(encoder_input_data.shape , max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb8d15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74473, 10) 10\n"
     ]
    }
   ],
   "source": [
    "# padding decoder_input_data\n",
    "padded_outputs = preprocessing.sequence.pad_sequences(outputs_int , maxlen=max_line_length, padding='post')\n",
    "decoder_input_data = np.array(padded_outputs)\n",
    "print(decoder_input_data.shape, max_line_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b168bff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74473, 10, 3879)\n"
     ]
    }
   ],
   "source": [
    "# padding decoder_output_data afrer removing <GO>\n",
    "for i in range(len(outputs_int)) :\n",
    "    outputs_int[i] = outputs_int[i][:-1]\n",
    "padded_outputs = preprocessing.sequence.pad_sequences(outputs_int, maxlen=max_line_length, padding='post')\n",
    "onehot_answers = utils.to_categorical(padded_outputs, len(outputs_vocab_to_int))\n",
    "decoder_output_data = np.array(onehot_answers)\n",
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c275556c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf019d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
